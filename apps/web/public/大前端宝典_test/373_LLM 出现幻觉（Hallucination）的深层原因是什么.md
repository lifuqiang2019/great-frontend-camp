# LLM 出现幻觉（Hallucination）的深层原因是什么

- **语言模型是概率模型，不是事实模型：** LLM 的本质是“预测下一个最可能的词”，不是在“查找真相”，而是在生成语言模式。当输入提示不明确或知识缺失时，会凭统计相关性“合理地编造”。
- **训练数据中存在噪声和虚假样本：** 大模型学习了互联网上的海量文本，而这些内容本身可能包含错误或臆测信息。模型学到这些偏差后，在回答中会自然复现。
- **缺乏事实验证机制：** 模型输出结果时不会自动校验真伪，也不会访问实时数据。在多轮推理中，错误会被“递进强化”——尤其是 Agent 模式下的反射循环，会放大错误逻辑。
- **Prompt 上下文过短或缺乏约束：** 当上下文被截断、知识片段不完整，模型会自动“补空缺”，生成符合语义但不符合事实的回答。
- **任务模糊或目标歧义：** 如果任务没有明确评价标准，模型会更倾向于填补内容空白，从而编造细节。
