# 前端实现 LLM 的流式输出。

使用 **Server-Sent Events (SSE)** ：可参考本文档网络板块的 SSE 实现。

这里介绍 OpenAI SDK ：

- 提供 stream: true 参数；
- 每生成一段内容就通过 data: 推送到前端。

```javascript
// 后端  
const response = await openai.chat.completions.create({  
model: "gpt-4o-mini",  
stream: true,  
messages: \[{ role: "user", content: "哲玄前端哲玄前端" }\],  
});  
for await (const chunk of response) {  
res.write(\`data: ${chunk.choices\[0\]?.delta?.content || ''}\\n\\n\`);  
}  
  
// 前端  
const eventSource = new EventSource("/api/chat");  
eventSource.onmessage = (e) => {  
appendToChat(e.data);  
};

```
