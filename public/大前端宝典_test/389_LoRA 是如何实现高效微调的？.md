# LoRA 是如何实现高效微调的？

LoRA（Low-Rank Adaptation）在原模型权重矩阵旁边插入两个低秩矩阵（A、B），在训练时只更新这两个小矩阵，原始权重冻结。相当于 W' = W + A × B 这样显存消耗下降 90%+，训练速度提升数倍。

LoRA 的核心思想是“用低维近似表达参数变化”。
